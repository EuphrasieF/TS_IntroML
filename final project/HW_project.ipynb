{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train sample from json file to pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_json('data/processed/train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract images from dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_angle = np.mean(df_train[df_train['inc_angle'] != 'na']['inc_angle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_angle(row):\n",
    "    angle = row['inc_angle']\n",
    "    float_angle = mean_angle if angle == 'na' else float(angle)\n",
    "    return np.pi / 180 * float_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        \n",
    "        sum_band = band_1 + band_2\n",
    "        \n",
    "        \n",
    "        #band_1 = (band_1 - band_1.min() - 0.5) / (band_1.max() - band_1.min())\n",
    "        #band_2 = (band_2 - band_2.min() - 0.5) / (band_2.max() - band_2.min())\n",
    "\n",
    "        imgs.append(np.array([band_1, band_2,\n",
    "                              #composite, \n",
    "                              sum_band, \n",
    "                             # prod_band_1, prod_band_2\n",
    "                             ]))\n",
    "\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = get_scaled_imgs(df_train), np.array(df_train['is_iceberg'], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHANNELS = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_more_data(X_train, y_train):\n",
    "    def rot90(img, n):\n",
    "        result = np.copy(img)    \n",
    "        for _ in range(n):\n",
    "            result = [np.rot90(result[channel]) for channel in range(len(result))]\n",
    "        return np.array(result)\n",
    "    \n",
    "    def flip(img):\n",
    "        return np.array([np.flip(img[channel], 0) for channel in range(len(img))])\n",
    "    \n",
    "    X_result = []\n",
    "    y_result = []\n",
    "    \n",
    "    for X, y in zip(X_train, y_train):\n",
    "        X_result.append(rot90(X, 0))\n",
    "        X_result.append(rot90(X, 1))\n",
    "        X_result.append(rot90(X, 2))\n",
    "        X_result.append(rot90(X, 3))\n",
    "        X_result.append(flip(rot90(X, 0)))\n",
    "        X_result.append(flip(rot90(X, 0)))\n",
    "        X_result.append(flip(rot90(X, 0)))\n",
    "        X_result.append(flip(rot90(X, 0)))\n",
    "        \n",
    "        for _ in range(8):\n",
    "            y_result.append(y)\n",
    "        \n",
    "        \n",
    "    return np.array(X_result), np.array(y_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = make_more_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12832, 3, 75, 75)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and validation parts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network. We use CNN with the following architecture: input -> (conv -> conv -> pool) x 3 -> dense -> dense. For conv layers, filter size is 3, stride is 1 everywhere except the first conv layer; for pool layers, pool size is 2. After last pool layer, resulting shape is batch_size x 256 channels x 1 x 1, then we feed it to dense layers. We use LeakyRectify as an activation function everywhere except the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import batch_norm\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "input_y = T.vector(\"y\", dtype='int32')\n",
    "input_shape = [None, CHANNELS, 75, 75]\n",
    "nl = L.nonlinearities.LeakyRectify()\n",
    "n = 8\n",
    "\n",
    "incoming = L.layers.InputLayer(shape=input_shape, input_var=input_X)\n",
    "incoming = batch_norm(L.layers.Conv2DLayer(incoming, num_filters=8*n, filter_size=3, stride=2, nonlinearity=nl))\n",
    "incoming = batch_norm(L.layers.Conv2DLayer(incoming, num_filters=8*n, filter_size=3, nonlinearity=nl))\n",
    "incoming = L.layers.Pool2DLayer(incoming, pool_size=2)\n",
    "#incoming = L.layers.DropoutLayer(incoming, p=0.3)\n",
    "incoming = batch_norm(L.layers.Conv2DLayer(incoming, num_filters=8*n, filter_size=3, nonlinearity=nl))\n",
    "incoming = batch_norm(L.layers.Conv2DLayer(incoming, num_filters=16*n, filter_size=3, nonlinearity=nl))\n",
    "incoming = L.layers.Pool2DLayer(incoming, pool_size=2)\n",
    "incoming = L.layers.DropoutLayer(incoming, p=0.3)\n",
    "incoming = batch_norm(L.layers.Conv2DLayer(incoming, num_filters=16*n, filter_size=3, nonlinearity=nl))\n",
    "incoming = batch_norm(L.layers.Conv2DLayer(incoming, num_filters=16*n, filter_size=3, nonlinearity=nl))\n",
    "incoming = L.layers.Pool2DLayer(incoming, pool_size=2)\n",
    "incoming = L.layers.DropoutLayer(incoming, p=0.3)\n",
    "incoming = L.layers.DenseLayer(incoming, num_units=16, nonlinearity=nl)\n",
    "#incoming = L.layers.DropoutLayer(incoming, p=0.3)\n",
    "incoming = L.layers.DenseLayer(incoming, num_units=1, nonlinearity=L.nonlinearities.sigmoid)\n",
    "\n",
    "y_predicted = L.layers.get_output(incoming)\n",
    "\n",
    "loss = L.objectives.binary_crossentropy(y_predicted, input_y).mean()\n",
    "accuracy = L.objectives.binary_accuracy(y_predicted, input_y).mean()\n",
    "\n",
    "updates = L.updates.adam(loss, L.layers.get_all_params(incoming, trainable=True))\n",
    "train_fn = theano.function([input_X, input_y], [loss, accuracy], updates=updates)\n",
    "val_fn = theano.function([input_X, input_y], [loss, accuracy])\n",
    "predict_fn = theano.function([input_X], y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batch_size):\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_index in range(0, len(X) - batch_size + 1, batch_size):\n",
    "        excerpt = indices[start_index:(start_index + batch_size)]\n",
    "\n",
    "        yield X[excerpt], y[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 352.521s\n",
      "  training loss (in-iteration):\t\t0.336889\n",
      "  train accuracy:\t\t84.17 %\n",
      "  validation accuracy:\t\t86.16 %\n",
      "  validation loss:\t\t0.286313\n",
      "Epoch 2 of 10 took 350.204s\n",
      "  training loss (in-iteration):\t\t0.239300\n",
      "  train accuracy:\t\t89.83 %\n",
      "  validation accuracy:\t\t91.44 %\n",
      "  validation loss:\t\t0.208778\n",
      "Epoch 3 of 10 took 351.679s\n",
      "  training loss (in-iteration):\t\t0.195487\n",
      "  train accuracy:\t\t92.08 %\n",
      "  validation accuracy:\t\t90.16 %\n",
      "  validation loss:\t\t0.232575\n",
      "Epoch 4 of 10 took 349.274s\n",
      "  training loss (in-iteration):\t\t0.156019\n",
      "  train accuracy:\t\t93.96 %\n",
      "  validation accuracy:\t\t93.12 %\n",
      "  validation loss:\t\t0.171717\n",
      "Epoch 5 of 10 took 348.346s\n",
      "  training loss (in-iteration):\t\t0.129953\n",
      "  train accuracy:\t\t95.20 %\n",
      "  validation accuracy:\t\t93.52 %\n",
      "  validation loss:\t\t0.158794\n",
      "Epoch 6 of 10 took 351.116s\n",
      "  training loss (in-iteration):\t\t0.104248\n",
      "  train accuracy:\t\t96.21 %\n",
      "  validation accuracy:\t\t93.84 %\n",
      "  validation loss:\t\t0.155944\n",
      "Epoch 7 of 10 took 348.221s\n",
      "  training loss (in-iteration):\t\t0.085140\n",
      "  train accuracy:\t\t96.82 %\n",
      "  validation accuracy:\t\t91.36 %\n",
      "  validation loss:\t\t0.202730\n",
      "Epoch 8 of 10 took 348.043s\n",
      "  training loss (in-iteration):\t\t0.077841\n",
      "  train accuracy:\t\t96.97 %\n",
      "  validation accuracy:\t\t93.28 %\n",
      "  validation loss:\t\t0.166457\n",
      "Epoch 9 of 10 took 348.463s\n",
      "  training loss (in-iteration):\t\t0.059497\n",
      "  train accuracy:\t\t97.88 %\n",
      "  validation accuracy:\t\t94.00 %\n",
      "  validation loss:\t\t0.192570\n",
      "Epoch 10 of 10 took 348.123s\n",
      "  training loss (in-iteration):\t\t0.046452\n",
      "  train accuracy:\t\t98.23 %\n",
      "  validation accuracy:\t\t94.08 %\n",
      "  validation loss:\t\t0.198694\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 20 #amount of passes through the data\n",
    "            \n",
    "batch_size = 50 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #in each epoch we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch = train_fn(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    val_acc = 0\n",
    "    val_err = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_err_batch, val_acc_batch = val_fn(inputs, targets)\n",
    "        val_acc += val_acc_batch\n",
    "        val_err += val_err_batch\n",
    "        val_batches += 1\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test dataframe, calculate results, save them in csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6986"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "#del X_train, y_train, X_val, y_val\n",
    "del df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('model.npz', *L.layers.get_all_param_values(incoming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('model.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "L.layers.set_all_param_values(incoming, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches_test(X, batch_size=500):\n",
    "    indices = np.arange(len(X))\n",
    "\n",
    "    for start_index in range(0, len(X), batch_size):\n",
    "        excerpt = indices[start_index:(start_index + batch_size)]\n",
    "\n",
    "        yield X[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_json('data/processed/test.json')\n",
    "X_test = get_scaled_imgs(df_test)\n",
    "\n",
    "y_test = np.array([])\n",
    "for batch in iterate_minibatches_test(X_test):\n",
    "    y_test = np.concatenate([y_test, predict_fn(batch).reshape(-1)])\n",
    "\n",
    "test_ans = df_test[['id']]\n",
    "test_ans['is_iceberg'] = y_test\n",
    "\n",
    "test_ans.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible improvements:\n",
    "* add another third channel in the input layer (need to understand some physics behind the process here)\n",
    "* scale data\n",
    "* play with CNN architecture: try residual connections, maxouts, fire modules; batchnorms, dropout layers; modify amount and parameters of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
